# PrivAI Embeddings Module

A comprehensive embedding generation module for PrivAI that uses sentence-transformers to create vector representations of text chunks for semantic search and similarity matching.

## Features

### üß† **Advanced Embedding Generation**
- **Sentence-Transformers**: Uses `all-MiniLM-L6-v2` model for high-quality embeddings
- **CPU Optimization**: Optimized for CPU execution with optional quantization
- **Vector Normalization**: Automatic normalization for cosine similarity search
- **Batch Processing**: Efficient batch processing for multiple chunks

### üíæ **Intelligent Caching**
- **Persistent Cache**: Saves embeddings to disk for reuse
- **Smart Cache Keys**: MD5-based cache keys for efficient lookups
- **Cache Management**: Automatic cache cleanup and statistics
- **Performance Boost**: Significant speedup for repeated operations

### üîç **Similarity Search**
- **Cosine Similarity**: Optimized cosine similarity computation
- **Top-K Search**: Find most similar chunks efficiently
- **Query Processing**: Generate embeddings for search queries
- **Semantic Matching**: Find relevant content by meaning, not just keywords

### ‚ö° **Performance Features**
- **Batch Processing**: Process multiple chunks simultaneously
- **Memory Efficient**: Optimized memory usage for large datasets
- **CPU Optimized**: Designed for CPU-only execution
- **Quantization Ready**: Support for quantized embeddings (future feature)

## Usage

### Basic Usage

```python
from app.core.embeddings import EmbeddingGenerator

# Initialize generator
generator = EmbeddingGenerator(
    model_name="all-MiniLM-L6-v2",
    device="cpu",
    use_quantization=False
)

# Generate embeddings for chunks
chunks_with_embeddings = generator.generate_embeddings(chunks)
```

### Convenience Functions

```python
from app.core.embeddings import generate_embeddings_for_chunks, generate_query_embedding

# Generate embeddings for chunks
chunks_with_embeddings = generate_embeddings_for_chunks(chunks)

# Generate query embedding
query_embedding = generate_query_embedding("What is PrivAI?")
```

### Similarity Search

```python
# Find similar chunks
similar_chunks = generator.find_similar_chunks(
    query_embedding,
    chunks_with_embeddings,
    top_k=5
)
```

### Batch Processing

```python
# Process in batches for efficiency
chunks_with_embeddings = generator.generate_embeddings(
    chunks,
    batch_size=32,
    use_cache=True
)
```

## Configuration

### Model Selection
- **Default**: `all-MiniLM-L6-v2` (384 dimensions, fast, good quality)
- **Alternative**: `all-mpnet-base-v2` (768 dimensions, higher quality)
- **Custom**: Any sentence-transformer model

### Device Configuration
```python
generator = EmbeddingGenerator(
    device="cpu",  # or "cuda" if available
    use_quantization=False  # for efficiency
)
```

### Cache Configuration
```python
generator = EmbeddingGenerator(
    cache_dir="path/to/cache",  # Custom cache directory
    use_cache=True  # Enable/disable caching
)
```

## API Reference

### EmbeddingGenerator Class

#### `__init__(model_name, device, use_quantization, cache_dir)`
Initialize the embedding generator.

**Parameters:**
- `model_name` (str): Sentence-transformer model name
- `device` (str): Device to run on ('cpu' or 'cuda')
- `use_quantization` (bool): Whether to use quantized embeddings
- `cache_dir` (str): Directory for caching embeddings

#### `generate_embeddings(chunks, batch_size, use_cache)`
Generate embeddings for a list of chunks.

**Parameters:**
- `chunks` (List[Dict]): List of chunk dictionaries
- `batch_size` (int): Batch size for processing
- `use_cache` (bool): Whether to use cached embeddings

**Returns:**
- `List[Dict]`: Chunks with added 'embedding' field

#### `find_similar_chunks(query_embedding, chunk_embeddings, top_k)`
Find most similar chunks to a query.

**Parameters:**
- `query_embedding` (np.ndarray): Query embedding vector
- `chunk_embeddings` (List[Dict]): Chunks with embeddings
- `top_k` (int): Number of top results to return

**Returns:**
- `List[Dict]`: Similar chunks with similarity scores

#### `compute_similarity(embedding1, embedding2)`
Compute cosine similarity between two embeddings.

**Parameters:**
- `embedding1` (np.ndarray): First embedding vector
- `embedding2` (np.ndarray): Second embedding vector

**Returns:**
- `float`: Cosine similarity score (0-1)

### Convenience Functions

#### `generate_embeddings_for_chunks(chunks, model_name, device, batch_size)`
Convenience function to generate embeddings.

#### `generate_query_embedding(query, model_name, device)`
Generate embedding for a single query string.

## Integration

### FastAPI Integration
The embeddings module integrates seamlessly with the PrivAI backend:

```python
from app.core.embeddings import get_default_embedding_generator

# Use default generator
generator = get_default_embedding_generator()
chunks_with_embeddings = generator.generate_embeddings(chunks)
```

### Vector Store Integration
The embeddings module is integrated with the FAISS vector store:

```python
from app.core.vector_store import vector_store

# Vector store automatically uses the embedding generator
vector_store.add_documents(documents)
```

## Performance

### Benchmarks
- **Model Loading**: ~2-3 seconds for all-MiniLM-L6-v2
- **Embedding Generation**: ~100-200 chunks/second (CPU)
- **Cache Hit**: ~10x faster than generation
- **Memory Usage**: ~50MB for model + embeddings

### Optimization Tips
1. **Use Caching**: Enable caching for repeated operations
2. **Batch Processing**: Use appropriate batch sizes (16-64)
3. **CPU Optimization**: Use CPU-optimized models
4. **Memory Management**: Process large datasets in batches

## Error Handling

The module includes comprehensive error handling:
- **Import Errors**: Graceful handling of missing dependencies
- **Model Loading**: Clear error messages for model issues
- **Memory Errors**: Handling of out-of-memory situations
- **Cache Errors**: Fallback when cache operations fail

## Dependencies

- `sentence-transformers`: Core embedding generation
- `numpy`: Vector operations
- `torch`: Optional for advanced features
- `pickle`: Cache persistence
- `hashlib`: Cache key generation

## Examples

See `examples/embeddings_examples.py` for comprehensive usage examples including:
- Basic embedding generation
- Similarity search
- Batch processing
- Caching demonstration
- Quality analysis
- JSON serialization

## Troubleshooting

### Common Issues

1. **Import Error**: Install sentence-transformers
   ```bash
   pip install sentence-transformers
   ```

2. **Memory Error**: Reduce batch size or use smaller model
   ```python
   generator = EmbeddingGenerator(batch_size=16)
   ```

3. **Cache Issues**: Clear cache or check permissions
   ```python
   generator.clear_cache()
   ```

### Performance Issues

1. **Slow Generation**: Enable caching and optimize batch size
2. **High Memory Usage**: Process in smaller batches
3. **Cache Misses**: Check cache directory permissions

## Future Enhancements

- **Quantization**: Support for quantized embeddings
- **GPU Acceleration**: CUDA support for faster processing
- **Model Fine-tuning**: Custom model training
- **Advanced Caching**: Distributed caching support
